COR GAZE DETECTION LIBRARY - TECHNICAL DOCUMENTATION
==================================================

VERSION: 1.0.1
LAST UPDATED: October 2024

TABLE OF CONTENTS
-----------------
1. OVERVIEW
2. INSTALLATION AND SETUP
3. API REFERENCE
4. CONFIGURATION FILES
5. ALGORITHMS AND METHODS
6. PERFORMANCE OPTIMIZATION
7. TROUBLESHOOTING
8. EXAMPLES AND USE CASES
9. DEVELOPMENT NOTES

1. OVERVIEW
-----------
Cor is a comprehensive gaze detection library implemented in Python using OpenCV and matplotlib. 
It provides automated eye tracking and gaze analysis capabilities for video processing 
applications. The library uses proven computer vision algorithms for robust detection 
across various lighting conditions and head poses.

Key Features:
- Multi-format video processing (MP4, AVI, MOV, MKV, WMV, FLV, WEBM)
- Automatic calibration system for eye and gaze detection
- Professional heatmap generation with multiple color schemes
- Video visualization with gaze tracking overlays
- Progress tracking with real-time updates during processing
- Pure Python implementation - no compilation required
- Cross-platform compatibility (Windows, macOS, Linux)
- Configurable detection parameters through text files

2. INSTALLATION AND SETUP
--------------------------

System Requirements:
- Operating System: Windows, macOS, Linux
- Python: 3.7 or higher
- OpenCV: 4.5 or higher (opencv-python package)
- NumPy: 1.19 or higher
- Matplotlib: 3.3 or higher
- Memory: Minimum 4GB RAM (8GB recommended for large videos)
- Storage: 100MB for library + space for output files

Installation Methods:

Method 1 - PyPI Installation:
    pip install cor

Method 2 - Source Installation:
    git clone https://github.com/victorcynzo/cor
    cd cor
    pip install -e .

Method 3 - Direct Installation:
    pip install opencv-python matplotlib numpy Pillow
    pip install -e .

Method 2 - Source Installation:
    git clone https://github.com/cor-team/cor.git
    cd cor
    pip install -e .

Method 3 - Development Installation:
    git clone https://github.com/cor-team/cor.git
    cd cor
    pip install -r requirements-dev.txt
    python setup.py build_ext --inplace

Method 4 - Automated Build and Test:
    python build_and_test.py

Verification:
    python -c "import cor; print(cor.__version__)"

3. API REFERENCE
----------------

3.1 CORE FUNCTIONS

cor.help()
    Description: Displays comprehensive help information for all available functions
    Parameters: None
    Returns: None (prints to stdout)
    Example: cor.help()

cor.calibrate_eyes(video_file)
    Description: Interactive eye detection calibration interface
    Parameters:
        video_file (str): Path to input video file
    Returns: None
    Side Effects: 
        - Creates/updates eye-detection-values.txt
        - Opens GUI calibration window
    Example: cor.calibrate_eyes("sample.mp4")
    
    Calibration Process:
        1. Extracts 20 evenly distributed frames from video
        2. Displays each frame with adjustable detection boundaries
        3. User adjusts ellipses/circles around eyes and pupils
        4. Saves detection parameters to configuration file
        5. Handles existing calibration data with merge/overwrite options

cor.calibrate_gaze(video_file)
    Description: Interactive gaze direction calibration interface
    Parameters:
        video_file (str): Path to input video file
    Returns: None
    Side Effects:
        - Creates/updates gaze-direction-values.txt
        - Opens GUI calibration window
    Example: cor.calibrate_gaze("sample.mp4")
    
    Calibration Process:
        1. Extracts 20 evenly distributed frames from video
        2. Displays frames with gaze direction indicator (green ball)
        3. Shows connecting lines between gaze point and pupils
        4. User adjusts gaze direction parameters
        5. Saves calibration data to configuration file

cor.run(video_file, *args)
    Description: Executes gaze detection analysis on video file
    Parameters:
        video_file (str): Path to input video file
        *args: Optional arguments (currently supports "--visualize")
    Returns: None
    Side Effects: Creates output files based on video name
    
    Standard Output Files:
        - {videoname}_heatmap-pure.jpg: Pure heatmap visualization
        - {videoname}_heatmap-overlay.jpg: Heatmap overlaid on 10th frame
    
    With --visualize Flag:
        - {videoname}_heatmap.{ext}: Full video with gaze overlay
    
    Examples:
        cor.run("video.mp4")
        cor.run("video.mp4", "--visualize")

cor.version()
    Description: Returns detailed version information
    Parameters: None
    Returns: Dictionary with version details
    Example: version_info = cor.version()

cor.get_config(param_name, config_file=None)
    Description: Retrieves configuration parameter value
    Parameters:
        param_name (str): Name of configuration parameter
        config_file (str, optional): Configuration file path
    Returns: String value of parameter
    Example: value = cor.get_config("heatmap_color_scheme")

cor.set_config(param_name, param_value, config_file=None)
    Description: Sets configuration parameter value
    Parameters:
        param_name (str): Name of configuration parameter
        param_value (str): Value to set
        config_file (str, optional): Configuration file path
    Returns: None
    Example: cor.set_config("heatmap_color_scheme", "sequential_red")

cor.validate_video(video_file)
    Description: Validates video file and returns properties
    Parameters:
        video_file (str): Path to video file
    Returns: Dictionary with validation results and video properties
    Example: info = cor.validate_video("sample.mp4")

cor.extract_frames(video_file, num_frames=5, output_dir="frames")
    Description: Extracts sample frames from video for preview
    Parameters:
        video_file (str): Path to video file
        num_frames (int): Number of frames to extract
        output_dir (str): Directory to save frames
    Returns: List of extracted frame filenames
    Example: frames = cor.extract_frames("video.mp4", 10)

cor.benchmark(video_file, max_frames=100)
    Description: Runs performance benchmark on video processing
    Parameters:
        video_file (str): Path to video file
        max_frames (int): Maximum frames to process for benchmark
    Returns: Dictionary with performance metrics
    Example: results = cor.benchmark("video.mp4")

cor.analyze_attention(video_file)
    Description: Analyzes attention patterns including fixations and saccades
    Parameters:
        video_file (str): Path to video file
    Returns: Dictionary with attention analysis results
    Example: analysis = cor.analyze_attention("video.mp4")

cor.generate_advanced_heatmap(video_file, mode="density", output_path=None)
    Description: Generates advanced heatmaps with specific visualization modes
    Parameters:
        video_file (str): Path to video file
        mode (str): Heatmap mode ("density", "fixation", "saccade")
        output_path (str, optional): Output file path
    Returns: None
    Example: cor.generate_advanced_heatmap("video.mp4", "fixation", "fixation_map.jpg")

cor.init_realtime(camera_id=0)
    Description: Initializes real-time camera processing
    Parameters:
        camera_id (int): Camera device ID (default: 0)
    Returns: Boolean indicating success
    Example: success = cor.init_realtime(0)

cor.process_realtime_frame()
    Description: Processes single frame from camera in real-time
    Parameters: None
    Returns: Dictionary with gaze point data
    Example: gaze_data = cor.process_realtime_frame()

cor.cleanup_realtime()
    Description: Cleans up real-time processing resources
    Parameters: None
    Returns: None
    Example: cor.cleanup_realtime()

cor.export_analysis(video_file, output_path=None)
    Description: Exports detailed analysis results to JSON file
    Parameters:
        video_file (str): Path to video file
        output_path (str, optional): Output JSON file path
    Returns: String path to exported file
    Example: export_path = cor.export_analysis("video.mp4")

3.2 INTERNAL FUNCTIONS (C++ IMPLEMENTATION)

detect_eyes(frame, calibration_data)
    Description: Detects eye regions in a single frame
    Implementation: Haar cascades + custom ellipse fitting
    Returns: Eye bounding boxes and pupil centers

detect_gaze_direction(eye_data, calibration_data)
    Description: Calculates gaze direction from eye detection data
    Implementation: Pupil center analysis + geometric calculations
    Returns: Gaze vector coordinates

generate_heatmap(gaze_data, frame_dimensions, color_scheme)
    Description: Creates heatmap from accumulated gaze data
    Implementation: Gaussian kernel density estimation
    Returns: Heatmap image array

4. CONFIGURATION FILES
----------------------

4.1 eye-detection-values.txt
Format: Key-value pairs, one per line
Purpose: Stores eye detection calibration parameters

Example Content:
    eye_cascade_scale_factor=1.1
    eye_cascade_min_neighbors=5
    eye_region_width_factor=0.3
    eye_region_height_factor=0.4
    pupil_detection_threshold=50
    pupil_min_radius=5
    pupil_max_radius=30
    left_eye_offset_x=0
    left_eye_offset_y=0
    right_eye_offset_x=0
    right_eye_offset_y=0

4.2 gaze-direction-values.txt
Format: Key-value pairs, one per line
Purpose: Stores gaze direction calibration parameters

Example Content:
    gaze_sensitivity_x=1.0
    gaze_sensitivity_y=1.0
    gaze_offset_x=0
    gaze_offset_y=0
    pupil_to_gaze_ratio=0.8
    gaze_smoothing_factor=0.3
    min_confidence_threshold=0.7
    max_gaze_angle=45

4.3 cor.txt
Format: Key-value pairs with descriptions
Purpose: General configuration and heatmap settings

Example Content:
    # Heatmap Configuration
    heatmap_color_scheme=sequential_blue
    # Options: sequential_blue, sequential_red, sequential_green, 
    #          diverging_blue_red, diverging_green_red, 
    #          categorical_5, categorical_7
    
    heatmap_intensity_multiplier=1.0
    # Controls overall heatmap brightness (0.1 - 5.0)
    
    heatmap_blur_radius=15
    # Gaussian blur radius for heatmap smoothing (5 - 50)
    
    heatmap_resolution_factor=1.0
    # Resolution multiplier for heatmap generation (0.5 - 2.0)
    
    # Video Processing
    frame_skip_factor=1
    # Process every Nth frame (1 = all frames, 2 = every other frame)
    
    output_video_quality=0.8
    # Video compression quality for --visualize output (0.1 - 1.0)
    
    # Visualization
    gaze_circle_radius=10
    # Radius of gaze indicator circle in pixels
    
    gaze_circle_color=0,255,0
    # RGB color values for gaze indicator (0-255 each)
    
    pupil_line_thickness=2
    # Thickness of lines connecting gaze point to pupils
    
    pupil_line_color=255,255,0
    # RGB color values for pupil connection lines

5. ALGORITHMS AND METHODS
-------------------------

5.1 EYE DETECTION ALGORITHM
- Primary Method: Haar Cascade Classifiers
- Backup Method: Template matching with normalized correlation
- Pupil Detection: Circular Hough Transform + intensity analysis
- Refinement: Ellipse fitting for improved accuracy

5.2 GAZE ESTIMATION ALGORITHM
- Method: Pupil center corneal reflection (PCCR) approximation
- Calibration: User-defined reference points for accuracy
- Smoothing: Temporal filtering to reduce noise
- Confidence Scoring: Quality assessment for each detection

5.3 HEATMAP GENERATION
- Density Estimation: 2D Gaussian kernel density estimation
- Normalization: Min-max scaling with configurable intensity
- Color Mapping: Multiple schemes with customizable palettes
- Overlay Blending: Alpha compositing for frame overlay

6. PERFORMANCE OPTIMIZATION
---------------------------

6.1 PROCESSING OPTIMIZATIONS
- Multi-threading: Parallel processing of video frames
- Memory Management: Efficient buffer allocation and reuse
- Frame Skipping: Configurable frame sampling for speed
- ROI Processing: Region of interest focusing for efficiency

6.2 PROGRESS TRACKING
- Real-time Progress Bars: Visual feedback for all video processing operations
- Unicode Block Characters: Clear progress visualization using █ characters
- Operation-specific Messages: Contextual status information for each process
- Frame-level Tracking: Detailed progress for video analysis operations
- Calibration Progress: Step-by-step progress during interactive calibration

6.3 CONFIDENCE ASSESSMENT
- Automatic Accuracy Evaluation: Post-processing confidence analysis
- Detection Rate Analysis: Percentage of successful gaze detections
- Confidence Distribution: Breakdown of high/medium/low confidence points
- Overall Accuracy Score: Composite reliability metric (0-100%)
- Interpretation Guidelines: Clear recommendations based on confidence levels
- Quality Recommendations: Suggestions for improving detection accuracy

6.4 MEMORY USAGE
- Typical Usage: 50-200MB for standard video processing
- Large Videos: Streaming processing to minimize memory footprint
- Calibration: Temporary storage of 20 frames (~100MB)

6.5 PERFORMANCE BENCHMARKS
- Standard Video (720p, 30fps): ~15-25 fps processing speed
- High Resolution (1080p+): ~8-15 fps processing speed
- Calibration Interface: Real-time interaction (30+ fps)

7. TROUBLESHOOTING
------------------

7.1 COMMON ISSUES

Issue: "OpenCV not found" error
Solution: Install OpenCV with pip install opencv-python

Issue: Poor eye detection accuracy
Solution: Run cor.calibrate_eyes() for video-specific tuning

Issue: Gaze direction seems incorrect
Solution: Use cor.calibrate_gaze() to set proper reference points

Issue: Video format not supported
Solution: Convert video to MP4 format using ffmpeg

Issue: Out of memory errors
Solution: Increase frame_skip_factor in cor.txt or use smaller videos

7.2 DEBUG MODE
Enable debug output by setting environment variable:
    export COR_DEBUG=1 (Linux/macOS)
    set COR_DEBUG=1 (Windows)

8. EXAMPLES AND USE CASES
-------------------------

8.1 RESEARCH APPLICATIONS
- Eye-tracking studies in psychology and neuroscience
- User interface usability testing
- Reading pattern analysis
- Attention and focus measurement

8.2 COMMERCIAL APPLICATIONS
- Marketing research and advertisement effectiveness
- Driver attention monitoring systems
- Accessibility technology development
- Gaming and virtual reality interfaces

8.3 SAMPLE WORKFLOWS

Basic Analysis Workflow:
    1. Import video file
    2. Run cor.run("video.mp4") for quick analysis
    3. Review heatmap outputs
    4. Adjust parameters if needed

Precision Analysis Workflow:
    1. Import video file
    2. Run cor.calibrate_eyes("video.mp4")
    3. Run cor.calibrate_gaze("video.mp4")
    4. Run cor.run("video.mp4", "--visualize")
    5. Review all outputs and fine-tune parameters

Batch Processing Workflow:
    1. Calibrate using representative video
    2. Process multiple videos with same settings
    3. Compare heatmap outputs across videos
    4. Generate summary statistics

9. DEVELOPMENT NOTES
--------------------

9.1 ARCHITECTURE
- Core Engine: C++ implementation for performance
- Python Bindings: Simplified interface for ease of use
- Configuration System: Text-based for user accessibility
- Modular Design: Separate components for different functions

9.2 IMPLEMENTED ADVANCED FEATURES
- Real-time camera input support with cor.init_realtime()
- Advanced attention pattern analysis with fixation and saccade detection
- Multiple heatmap visualization modes (density, fixation, saccade)
- JSON export of detailed analysis results
- Performance benchmarking and optimization tools

9.3 FUTURE ENHANCEMENTS
- Machine learning-based eye detection
- 3D gaze vector estimation
- Multi-person tracking capabilities
- Cloud processing integration
- Mobile device support

9.3 CONTRIBUTING
See CONTRIBUTING.md for development guidelines and coding standards.

9.4 LICENSE
MIT License - see LICENSE file for full terms.

---
For additional support, visit: https://github.com/cor-team/cor
Documentation updates: https://cor-gaze.readthedocs.io/